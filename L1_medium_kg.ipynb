{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44158fd0-0622-4546-80a5-861988b96b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "045d7bf1-2595-4014-a4d9-f5db0e675e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install neo4j\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eee1c0d3-8e2e-4c15-b43f-99adafcc38d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e2e15-adcb-4f3f-b716-8b19007bfcf7",
   "metadata": {},
   "source": [
    "## Setup NEO4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f34fea-87ea-494d-8f88-b61380a0dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = os.environ[\"NEO4J_USERNAME\"]\n",
    "password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "graph = Neo4jGraph(url = url, username = username, password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a392d1-320e-41c6-af09-a6c27779e158",
   "metadata": {},
   "source": [
    "## Creating data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e29f3f-521b-40f9-971f-05061fb7894f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "import_url = \"https://gist.githubusercontent.com/tomasonjo/08dc8ba0e19d592c4c3cde40dd6abcc3/raw/da8882249af3e819a80debf3160ebbb3513ee962/microservices.json\"\n",
    "import_query = requests.get(import_url).json()['query']\n",
    "graph.query(\n",
    "    import_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef26a69b-823a-4fbe-801f-f01fd78c8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a59b55bf-f72b-433b-9f13-a9c1bf3572de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing vector index search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61bbe5e-00fa-4138-a715-bef29d01eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name: RecommendationFeature\n",
      "description: Add feature to RecommendationService\n",
      "status: In Progress\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    index_name='tasks',\n",
    "    node_label=\"Task\",\n",
    "    text_node_properties=['name', 'description', 'status'],\n",
    "    embedding_node_property='embedding',\n",
    ")\n",
    "\n",
    "response = vector_index.similarity_search(\"How will RecommendationService be updated?\")\n",
    "print(response[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2215e5b1-d650-455c-9c0d-41a43759ad31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(*)': 0}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "vector_qa = RetrievalQA.from_chain_type(llm = ChatOpenAI(), chain_type = \"stuff\", retriever = vector_index.as_retriever())\n",
    "vector_qa.invoke({\"query\": \"How will recommendation service be updated?\"})\n",
    "vector_qa.invoke(\n",
    "    {\"query\": \"How many open tickets there are?\"}\n",
    ")\n",
    "graph.query(\n",
    "    \"MATCH (t: Task{status: 'open'}) RETURN count(*)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295bb9cb-60b2-4a24-a61c-c4b6845bb01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (t:Task {status: 'open'}) RETURN COUNT(t)\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'COUNT(t)': 0}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (t:Task)-[:ASSIGNED_TO]->(team:Team)\n",
      "WHERE t.status = 'open'\n",
      "RETURN team.name, COUNT(t) AS OpenTasks\n",
      "ORDER BY OpenTasks DESC\n",
      "LIMIT 1\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Microservice)-[:DEPENDS_ON]->(d:Dependency {type: 'Database'}) RETURN m.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Microservice)-[:DEPENDS_ON]->(:Dependency {name: 'Database'}) RETURN m.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'m.name': 'PaymentService'}, {'m.name': 'UserService'}, {'m.name': 'AuthService'}, {'m.name': 'OrderService'}, {'m.name': 'CatalogService'}, {'m.name': 'InventoryService'}, {'m.name': 'PaymentService'}, {'m.name': 'OrderService'}, {'m.name': 'CatalogService'}, {'m.name': 'AuthService'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Which services depend on Database indirectly?',\n",
       " 'result': 'PaymentService, UserService, AuthService, OrderService, CatalogService, InventoryService depend on Database indirectly.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "\n",
    "graph.refresh_schema()\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    cypher_llm = ChatOpenAI(temperature=0, model_name='gpt-4'),\n",
    "    qa_llm = ChatOpenAI(temperature=0),\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "cypher_chain.invoke({\"query\": \"How many open tickets are there?\"})\n",
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Which team has the most open tasks?\"}\n",
    ")\n",
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Which services depend on Database directly?\"}\n",
    ")\n",
    "cypher_chain.invoke(\n",
    "    {\"query\": \"Which services depend on Database indirectly?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0529cc6b-9004-426a-aad0-201d243987b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fde880c-2984-4484-9420-17d708bad0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'input'] input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Graph` with `Which team is assigned to maintain PaymentService?`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Microservice {name: 'PaymentService'})-[:MAINTAINED_BY]->(t:Team) RETURN t.name\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'t.name': 'TeamD'}, {'t.name': 'TeamD'}, {'t.name': 'TeamD'}, {'t.name': 'TeamD'}, {'t.name': 'TeamD'}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{'query': 'Which team is assigned to maintain PaymentService?', 'result': 'TeamD is assigned to maintain PaymentService.'}\u001b[0m\u001b[32;1m\u001b[1;3mTeamD is assigned to maintain PaymentService.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Which team is assigned to maintain PaymentService?', 'output': 'TeamD is assigned to maintain PaymentService.'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Tasks` with `Which tasks have optimization in their description?`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m{'query': 'Which tasks have optimization in their description?', 'result': 'All tasks have optimization in their description.'}\u001b[0m\u001b[32;1m\u001b[1;3mAll tasks have optimization in their description.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Which tasks have optimization in their description?', 'output': 'All tasks have optimization in their description.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_openai_functions_agent, Tool, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Tasks\",\n",
    "        func=vector_qa.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about descriptions of tasks.\n",
    "        Not useful for counting the number of tasks.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Graph\",\n",
    "        func=cypher_chain.invoke,\n",
    "        description=\"\"\"Useful when you need to answer questions about microservices,\n",
    "        their dependencies or assigned people. Also useful for any sort of\n",
    "        aggregation like counting the number of tasks, etc.\n",
    "        Use full question as input.\n",
    "        \"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "print(prompt)\n",
    "agent = create_openai_functions_agent(\n",
    "    ChatOpenAI(temperature=0, model_name='gpt-4'), tools, prompt\n",
    ")\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "response = agent_executor.invoke({\"input\": \"Which team is assigned to maintain PaymentService?\"})\n",
    "print(response)\n",
    "response = agent_executor.invoke({\"input\": \"Which tasks have optimization in their description?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473919f3-91bb-4969-8de0-89bc4a04a7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
